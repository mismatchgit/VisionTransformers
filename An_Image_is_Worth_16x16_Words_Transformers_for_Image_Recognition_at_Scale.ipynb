{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyM3myccPCXErE3MNKwZTVPq",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mismatchgit/VisionTransformers/blob/main/An_Image_is_Worth_16x16_Words_Transformers_for_Image_Recognition_at_Scale.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.utils.data as data\n",
        "import math\n",
        "import copy"
      ],
      "metadata": {
        "id": "SML9HPr0TJGM"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class which subclasses nn.Module\n",
        "class PatchEmbedding(nn.Module):\n",
        "    \"\"\"Turns a 2D input image into a 1D sequence learnable embedding vector.\n",
        "\n",
        "    Args:\n",
        "        in_channels (int): Number of color channels for the input images. Defaults to 3.\n",
        "        patch_size (int): Size of patches to convert input image into. Defaults to 16.\n",
        "        embedding_dim (int): Size of embedding to turn image into. Defaults to 768.\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with appropriate variables\n",
        "    def __init__(self,\n",
        "                 in_channels:int=3,\n",
        "                 patch_size:int=16,\n",
        "                 embedding_dim:int=768):\n",
        "        super().__init__()\n",
        "\n",
        "        # Store patch_size as an instance attribute\n",
        "        self.patch_size = patch_size\n",
        "\n",
        "        # 3. Create a layer to turn an image into patches\n",
        "        self.patcher = nn.Conv2d(in_channels=in_channels,\n",
        "                                 out_channels=embedding_dim,\n",
        "                                 kernel_size=patch_size,\n",
        "                                 stride=patch_size,\n",
        "                                 padding=0)\n",
        "\n",
        "        # 4. Create a layer to flatten the patch feature maps into a single dimension\n",
        "        self.flatten = nn.Flatten(start_dim=2, # only flatten the feature map dimensions into a single vector\n",
        "                                  end_dim=3)\n",
        "\n",
        "    # 5. Define the forward method\n",
        "    def forward(self, x):\n",
        "        # Create assertion to check that inputs are the correct shape\n",
        "        image_resolution = x.shape[-1]\n",
        "        assert image_resolution % self.patch_size == 0, f\"Input image size must be divisible by patch size, image shape: {image_resolution}, patch size: {self.patch_size}\"\n",
        "\n",
        "        # Perform the forward pass\n",
        "        x_patched = self.patcher(x)\n",
        "        x_flattened = self.flatten(x_patched)\n",
        "        # 6. Make sure the output shape has the right order\n",
        "        return x_flattened.permute(0, 2, 1) # adjust so the embedding is on the final dimension [batch_size, P^2•C, N] -> [batch_size, N, P^2•C]"
      ],
      "metadata": {
        "id": "p2Vpax8fS_7h"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MultiheadSelfAttentionBlock(nn.Module):\n",
        "    \"\"\"Creates a multi-head self-attention block (\"MSA block\" for short).\n",
        "    \"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0): # doesn't look like the paper uses any dropout in MSABlocks\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multi-Head Attention (MSA) layer\n",
        "        self.multihead_attn = nn.MultiheadAttention(embed_dim=embedding_dim,\n",
        "                                                    num_heads=num_heads,\n",
        "                                                    dropout=attn_dropout,\n",
        "                                                    batch_first=True) # does our batch dimension come first?\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        attn_output, _ = self.multihead_attn(query=x, # query embeddings\n",
        "                                             key=x, # key embeddings\n",
        "                                             value=x, # value embeddings\n",
        "                                             need_weights=False) # do we need the weights or just the layer outputs?\n",
        "        return attn_output\n",
        ""
      ],
      "metadata": {
        "id": "80U618iHRLWx"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class MLPBlock(nn.Module):\n",
        "    \"\"\"Creates a layer normalized multilayer perceptron block (\"MLP block\" for short).\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden Size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 dropout:float=0.1): # Dropout from Table 3 for ViT-Base\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create the Norm layer (LN)\n",
        "        self.layer_norm = nn.LayerNorm(normalized_shape=embedding_dim)\n",
        "\n",
        "        # 4. Create the Multilayer perceptron (MLP) layer(s)\n",
        "        self.mlp = nn.Sequential(\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=mlp_size),\n",
        "            nn.GELU(), # \"The MLP contains two layers with a GELU non-linearity (section 3.1).\"\n",
        "            nn.Dropout(p=dropout),\n",
        "            nn.Linear(in_features=mlp_size, # needs to take same in_features as out_features of layer above\n",
        "                      out_features=embedding_dim), # take back to embedding_dim\n",
        "            nn.Dropout(p=dropout) # \"Dropout, when used, is applied after every dense layer..\"\n",
        "        )\n",
        "\n",
        "    # 5. Create a forward() method to pass the data through the layers\n",
        "    def forward(self, x):\n",
        "        x = self.layer_norm(x)\n",
        "        x = self.mlp(x)\n",
        "        return x"
      ],
      "metadata": {
        "id": "1VUGyRliUOlT"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a class that inherits from nn.Module\n",
        "class TransformerEncoderBlock(nn.Module):\n",
        "    \"\"\"Creates a Transformer Encoder block.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 mlp_dropout:float=0.1, # Amount of dropout for dense layers from Table 3 for ViT-Base\n",
        "                 attn_dropout:float=0): # Amount of dropout for attention layers\n",
        "        super().__init__()\n",
        "\n",
        "        # 3. Create MSA block (equation 2)\n",
        "        self.msa_block = MultiheadSelfAttentionBlock(embedding_dim=embedding_dim,\n",
        "                                                     num_heads=num_heads,\n",
        "                                                     attn_dropout=attn_dropout)\n",
        "\n",
        "        # 4. Create MLP block (equation 3)\n",
        "        self.mlp_block =  MLPBlock(embedding_dim=embedding_dim,\n",
        "                                   mlp_size=mlp_size,\n",
        "                                   dropout=mlp_dropout)\n",
        "\n",
        "    # 5. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 6. Create residual connection for MSA block (add the input to the output)\n",
        "        x =  self.msa_block(x) + x\n",
        "\n",
        "        # 7. Create residual connection for MLP block (add the input to the output)\n",
        "        x = self.mlp_block(x) + x\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "xyzTRlKjTV9a"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Paper Link:** https://arxiv.org/pdf/2010.11929"
      ],
      "metadata": {
        "id": "N0_yLnr3PHzR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1.  Create a class called `ViT` that inherits from `torch.nn.Module`.\n",
        "2.  Initialize the class with hyperparameters from Table 1 and Table 3 of the ViT paper for the ViT-Base model.\n",
        "3.  Make sure the image size is divisible by the patch size (the image should be split into even patches).\n",
        "4.  Calculate the number of patches using the formula $N=H W / P^{2}$, where $H$ is the image height, $W$ is the image width and $P$ is the patch size.\n",
        "5.  Create a learnable class embedding token (equation 1).\n",
        "6.  Create a learnable position embedding vector (equation 1).\n",
        "7.  Setup the embedding dropout layer as discussed in Appendix B.1 of the ViT paper.\n",
        "8.  Create the patch embedding layer using the `PatchEmbedding` class 5.\n",
        "9.  Create a series of Transformer Encoder blocks by passing a list of `TransformerEncoderBlocks` created earlier to `torch.nn.Sequential()` (equations 2 & 3).\n",
        "10. Create the MLP head (also called classifier head or equation 4) by passing a `torch.nn.LayerNorm()` (LN) layer and a `torch.nn.Linear(out_features=num_classes)` layer (where `num_classes` is the target number of classes) linear layer to `torch.nn.Sequential()`.\n",
        "11. Create a `forward()` method that accepts an input.\n",
        "12. Get the batch size of the input (the first dimension of the shape).\n",
        "13. Create the patching embedding using the layer (equation 1).\n",
        "14. Create the class token embedding using the layer created in step 5 and expand it across the number of batches found in step 11 using `torch.Tensor.expand()` (equation 1).\n",
        "15. Concatenate the class token embedding created in step 13 to the first dimension of the patch embedding created in step 12 using `torch.cat()` (equation 1).\n",
        "16. Add the position embedding created in step 6 to the patch and class token embedding created in step 14 (equation 1).\n",
        "17. Pass the patch and position embedding through the dropout layer created in step 7.\n",
        "18. Pass the patch and position embedding from step 16 through the stack of Transformer Encoder layers created in step 9 (equations 2 & 3).\n",
        "19. Pass index 0 of the output of the stack of Transformer Encoder layers from step 17 through the classifier head created in step 10 (equation 4).\n",
        "20. We just built a Vision Transformer!\n",
        "---"
      ],
      "metadata": {
        "id": "AH4kN91TOCC2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "10MbcZH2S9Ws"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Create a ViT class that inherits from nn.Module\n",
        "class ViT(nn.Module):\n",
        "    \"\"\"Creates a Vision Transformer architecture with ViT-Base hyperparameters by default.\"\"\"\n",
        "    # 2. Initialize the class with hyperparameters from Table 1 and Table 3\n",
        "    def __init__(self,\n",
        "                 img_size:int=224, # Training resolution from Table 3 in ViT paper\n",
        "                 in_channels:int=3, # Number of channels in input image\n",
        "                 patch_size:int=16, # Patch size\n",
        "                 num_transformer_layers:int=12, # Layers from Table 1 for ViT-Base\n",
        "                 embedding_dim:int=768, # Hidden size D from Table 1 for ViT-Base\n",
        "                 mlp_size:int=3072, # MLP size from Table 1 for ViT-Base\n",
        "                 num_heads:int=12, # Heads from Table 1 for ViT-Base\n",
        "                 attn_dropout:float=0, # Dropout for attention projection\n",
        "                 mlp_dropout:float=0.1, # Dropout for dense/MLP layers\n",
        "                 embedding_dropout:float=0.1, # Dropout for patch and position embeddings\n",
        "                 num_classes:int=1000): # Default for ImageNet but can customize this\n",
        "        super().__init__() # don't forget the super().__init__()!\n",
        "\n",
        "        # 3. Make the image size is divisible by the patch size\n",
        "        assert img_size % patch_size == 0, f\"Image size must be divisible by patch size, image size: {img_size}, patch size: {patch_size}.\"\n",
        "\n",
        "        # 4. Calculate number of patches (height * width/patch^2)\n",
        "        self.num_patches = (img_size * img_size) // patch_size**2\n",
        "\n",
        "        # 5. Create learnable class embedding (needs to go at front of sequence of patch embeddings)\n",
        "        self.class_embedding = nn.Parameter(data=torch.randn(1, 1, embedding_dim),\n",
        "                                            requires_grad=True)\n",
        "\n",
        "        # 6. Create learnable position embedding\n",
        "        self.position_embedding = nn.Parameter(data=torch.randn(1, self.num_patches+1, embedding_dim),\n",
        "                                               requires_grad=True)\n",
        "\n",
        "        # 7. Create embedding dropout value\n",
        "        self.embedding_dropout = nn.Dropout(p=embedding_dropout)\n",
        "\n",
        "        # 8. Create patch embedding layer\n",
        "        self.patch_embedding = PatchEmbedding(in_channels=in_channels,\n",
        "                                              patch_size=patch_size,\n",
        "                                              embedding_dim=embedding_dim)\n",
        "\n",
        "        # 9. Create Transformer Encoder blocks (we can stack Transformer Encoder blocks using nn.Sequential())\n",
        "        # Note: The \"*\" means \"all\"\n",
        "        self.transformer_encoder = nn.Sequential(*[TransformerEncoderBlock(embedding_dim=embedding_dim,\n",
        "        num_heads=num_heads,\n",
        "        mlp_size=mlp_size,\n",
        "        mlp_dropout=mlp_dropout) for _ in range(num_transformer_layers)])\n",
        "\n",
        "        # 10. Create classifier head\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.LayerNorm(normalized_shape=embedding_dim),\n",
        "            nn.Linear(in_features=embedding_dim,\n",
        "                      out_features=num_classes)\n",
        "        )\n",
        "\n",
        "    # 11. Create a forward() method\n",
        "    def forward(self, x):\n",
        "\n",
        "        # 12. Get batch size\n",
        "        batch_size = x.shape[0]\n",
        "\n",
        "        # 13. Create class token embedding and expand it to match the batch size (equation 1)\n",
        "        class_token = self.class_embedding.expand(batch_size, -1, -1) # \"-1\" means to infer the dimension (try this line on its own)\n",
        "\n",
        "        # 14. Create patch embedding (equation 1)\n",
        "        x = self.patch_embedding(x)\n",
        "\n",
        "        # 15. Concat class embedding and patch embedding (equation 1)\n",
        "        x = torch.cat((class_token, x), dim=1)\n",
        "\n",
        "        # 16. Add position embedding to patch embedding (equation 1)\n",
        "        x = self.position_embedding + x\n",
        "\n",
        "        # 17. Run embedding dropout (Appendix B.1)\n",
        "        x = self.embedding_dropout(x)\n",
        "\n",
        "        # 18. Pass patch, position and class embedding through transformer encoder layers (equations 2 & 3)\n",
        "        x = self.transformer_encoder(x)\n",
        "\n",
        "        # 19. Put 0 index logit through classifier (equation 4)\n",
        "        x = self.classifier(x[:, 0]) # run on each sample in a batch at 0 index\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "a8mDzJpLO0aa"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# set_seeds()\n",
        "\n",
        "# Create a random tensor with same shape as a single image\n",
        "random_image_tensor = torch.randn(1, 3, 224, 224) # (batch_size, color_channels, height, width)\n",
        "\n",
        "# Create an instance of ViT with the number of classes we're working with (pizza, steak, sushi)\n",
        "# vit = ViT(num_classes=len(class_names))\n",
        "vit = ViT(num_classes=3)\n",
        "\n",
        "# Pass the random image tensor to our ViT instance\n",
        "vit(random_image_tensor)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQm9cKBqVRHX",
        "outputId": "94ea46b4-9d35-4290-b4d9-4d0b458db19b"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.7449, 0.0069, 0.4066]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torchinfo"
      ],
      "metadata": {
        "id": "D5Q1UN3rVEr1"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Print a summary of our custom ViT model using torchinfo (uncomment for actual output)\n",
        "summary(model=vit,\n",
        "        input_size=(32, 3, 224, 224), # (batch_size, color_channels, height, width)\n",
        "        # col_names=[\"input_size\"], # uncomment for smaller output\n",
        "        col_names=[\"input_size\", \"output_size\", \"num_params\", \"trainable\"],\n",
        "        col_width=20,\n",
        "        row_settings=[\"var_names\"]\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pz0PtEueTZ2l",
        "outputId": "f3c72098-9b81-447c-a666-5f68204f6456"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "============================================================================================================================================\n",
              "Layer (type (var_name))                                      Input Shape          Output Shape         Param #              Trainable\n",
              "============================================================================================================================================\n",
              "ViT (ViT)                                                    [32, 3, 224, 224]    [32, 3]              152,064              True\n",
              "├─PatchEmbedding (patch_embedding)                           [32, 3, 224, 224]    [32, 196, 768]       --                   True\n",
              "│    └─Conv2d (patcher)                                      [32, 3, 224, 224]    [32, 768, 14, 14]    590,592              True\n",
              "│    └─Flatten (flatten)                                     [32, 768, 14, 14]    [32, 768, 196]       --                   --\n",
              "├─Dropout (embedding_dropout)                                [32, 197, 768]       [32, 197, 768]       --                   --\n",
              "├─Sequential (transformer_encoder)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    └─TransformerEncoderBlock (0)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (1)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (2)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (3)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (4)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (5)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (6)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (7)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (8)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (9)                           [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (10)                          [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "│    └─TransformerEncoderBlock (11)                          [32, 197, 768]       [32, 197, 768]       --                   True\n",
              "│    │    └─MultiheadSelfAttentionBlock (msa_block)          [32, 197, 768]       [32, 197, 768]       2,363,904            True\n",
              "│    │    └─MLPBlock (mlp_block)                             [32, 197, 768]       [32, 197, 768]       4,723,968            True\n",
              "├─Sequential (classifier)                                    [32, 768]            [32, 3]              --                   True\n",
              "│    └─LayerNorm (0)                                         [32, 768]            [32, 768]            1,536                True\n",
              "│    └─Linear (1)                                            [32, 768]            [32, 3]              2,307                True\n",
              "============================================================================================================================================\n",
              "Total params: 85,800,963\n",
              "Trainable params: 85,800,963\n",
              "Non-trainable params: 0\n",
              "Total mult-adds (Units.GIGABYTES): 5.52\n",
              "============================================================================================================================================\n",
              "Input size (MB): 19.27\n",
              "Forward/backward pass size (MB): 3292.20\n",
              "Params size (MB): 229.20\n",
              "Estimated Total Size (MB): 3540.67\n",
              "============================================================================================================================================"
            ]
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    }
  ]
}